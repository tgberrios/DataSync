2025-09-27 14:26:38.874[ERROR][TRANSFER][performBulkUpsert]Skipping problematic record in batch: ERROR: current transaction is aborted, commands ignored until end of transaction block
2025-09-27 14:26:38.874[ERROR][TRANSFER][performBulkUpsert]Skipping problematic record in batch: ERROR: current transaction is aborted, commands ignored until end of transaction block
2025-09-27 14:26:38.874[ERROR][TRANSFER][performBulkUpsert]Skipping problematic record in batch: ERROR: current transaction is aborted, commands ignored until end of transaction block
2025-09-27 14:26:38.874[ERROR][TRANSFER][performBulkUpsert]Skipping problematic record in batch: ERROR: current transaction is aborted, commands ignored until end of transaction block
2025-09-27 14:26:38.874[ERROR][TRANSFER][performBulkUpsert]Skipping problematic record in batch: ERROR: current transaction is aborted, commands ignored until end of transaction block
2025-09-27 14:26:38.874[ERROR][TRANSFER][performBulkUpsert]Skipping problematic record in batch: ERROR: current transaction is aborted, commands ignored until end of transaction block
2025-09-27 14:26:38.873[ERROR][TRANSFER][performBulkUpsert]Skipping problematic record in batch: ERROR: current transaction is aborted, commands ignored until end of transaction block
2025-09-27 14:26:38.873[ERROR][TRANSFER][performBulkUpsert]Skipping problematic record in batch: ERROR: current transaction is aborted, commands ignored until end of transaction block
2025-09-27 14:26:38.873[ERROR][TRANSFER][performBulkUpsert]Skipping problematic record in batch: ERROR: current transaction is aborted, commands ignored until end of transaction block
2025-09-27 14:26:38.873[ERROR][TRANSFER][performBulkUpsert]Skipping problematic record in batch: ERROR: current transaction is aborted, commands ignored until end of transaction block
2025-09-27 14:26:38.873[ERROR][TRANSFER][performBulkUpsert]Skipping problematic record in batch: ERROR: current transaction is aborted, commands ignored until end of transaction block
2025-09-27 14:26:38.873[ERROR][TRANSFER][performBulkUpsert]Skipping problematic record in batch: ERROR: current transaction is aborted, commands ignored until end of transaction block
2025-09-27 14:26:38.873[ERROR][TRANSFER][performBulkUpsert]Skipping problematic record in batch: ERROR: current transaction is aborted, commands ignored until end of transaction block
2025-09-27 14:26:38.873[ERROR][TRANSFER][performBulkUpsert]Skipping problematic record in batch: ERROR: current transaction is aborted, commands ignored until end of transaction block
2025-09-27 14:26:38.873[ERROR][TRANSFER][performBulkUpsert]Skipping problematic record in batch: ERROR: current transaction is aborted, commands ignored until end of transaction block



2025-09-27 14:29:31.810[ERROR][SYSTEM][GOVERNANCE]Error storing metadata: ERROR: value too long for type character varying(200)
2025-09-27 14:29:31.724[ERROR][SYSTEM][GOVERNANCE]Error storing metadata: ERROR: value too long for type character varying(200)
2025-09-27 14:29:31.715[ERROR][SYSTEM][GOVERNANCE]Error storing metadata: ERROR: value too long for type character varying(200)
2025-09-27 14:29:31.629[ERROR][SYSTEM][GOVERNANCE]Error storing metadata: ERROR: value too long for type character varying(200)
2025-09-27 14:29:31.346[ERROR][SYSTEM][GOVERNANCE]Error storing metadata: ERROR: value too long for type character varying(200)
2025-09-27 14:29:31.261[ERROR][SYSTEM][GOVERNANCE]Error storing metadata: ERROR: value too long for type character varying(200)
2025-09-27 14:29:31.173[ERROR][SYSTEM][GOVERNANCE]Error storing metadata: ERROR: value too long for type character varying(200)
2025-09-27 14:29:30.994[ERROR][SYSTEM][GOVERNANCE]Error storing metadata: ERROR: value too long for type character varying(200)
2025-09-27 14:29:30.908[ERROR][SYSTEM][GOVERNANCE]Error storing metadata: ERROR: value too long for type character varying(200)
2025-09-27 14:29:30.900[ERROR][SYSTEM][GOVERNANCE]Error storing metadata: ERROR: value too long for type character varying(200)
2025-09-27 14:29:30.814[ERROR][SYSTEM][GOVERNANCE]Error storing metadata: ERROR: value too long for type character varying(200)
2025-09-27 14:29:12.962[ERROR][SYSTEM][GOVERNANCE]Error storing metadata: ERROR: value too long for type character varying(200)
2025-09-27 14:29:12.879[ERROR][SYSTEM][GOVERNANCE]Error storing metadata: ERROR: value too long for type character varying(200)
2025-09-27 14:29:12.871[ERROR][SYSTEM][GOVERNANCE]Error storing metadata: ERROR: value too long for type character varying(200)
2025-09-27 14:29:12.787[ERROR][SYSTEM][GOVERNANCE]Error storing metadata: ERROR: value too long for type character varying(200)
2025-09-27 14:27:37.889[ERROR][SYSTEM][GOVERNANCE]Error analyzing table structure: ERROR: improper relation name (too many dotted names): monitoringservice.jetu.cr.warning_messages
2025-09-27 14:27:37.865[ERROR][SYSTEM][GOVERNANCE]Error analyzing table structure: ERROR: improper relation name (too many dotted names): monitoringservice.jetu.cr.warning_definitions
2025-09-27 14:27:37.842[ERROR][SYSTEM][GOVERNANCE]Error analyzing table structure: ERROR: improper relation name (too many dotted names): monitoringservice.jetu.cr.users
2025-09-27 14:27:37.818[ERROR][SYSTEM][GOVERNANCE]Error analyzing table structure: ERROR: improper relation name (too many dotted names): monitoringservice.jetu.cr.user_message
2025-09-27 14:27:37.794[ERROR][SYSTEM][GOVERNANCE]Error analyzing table structure: ERROR: improper relation name (too many dotted names): monitoringservice.jetu.cr.user_list
2025-09-27 14:27:37.770[ERROR][SYSTEM][GOVERNANCE]Error analyzing table structure: ERROR: improper relation name (too many dotted names): monitoringservice.jetu.cr.transaction_statistics_profile
2025-09-27 14:27:37.746[ERROR][SYSTEM][GOVERNANCE]Error analyzing table structure: ERROR: improper relation name (too many dotted names): monitoringservice.jetu.cr.transaction_statistics_base_profile
2025-09-27 14:27:37.722[ERROR][SYSTEM][GOVERNANCE]Error analyzing table structure: ERROR: improper relation name (too many dotted names): monitoringservice.jetu.cr.transaction_statistics_base
2025-09-27 14:27:37.697[ERROR][SYSTEM][GOVERNANCE]Error analyzing table structure: ERROR: improper relation name (too many dotted names): monitoringservice.jetu.cr.transaction_statistics
2025-09-27 14:27:37.674[ERROR][SYSTEM][GOVERNANCE]Error analyzing table structure: ERROR: improper relation name (too many dotted names): monitoringservice.jetu.cr.password_resets
2025-09-27 14:27:37.650[ERROR][SYSTEM][GOVERNANCE]Error analyzing table structure: ERROR: improper relation name (too many dotted names): monitoringservice.jetu.cr.migrations
2025-09-27 14:27:37.626[ERROR][SYSTEM][GOVERNANCE]Error analyzing table structure: ERROR: improper relation name (too many dotted names): monitoringservice.jetu.cr.definition_message


2025-09-27 14:34:03.833[ERROR][SYSTEM][TRANSFER]CRITICAL ERROR: Bulk upsert failed for chunk 2063 in table crm.prospect_lists_prospects: ERROR: ON CONFLICT DO UPDATE command cannot affect row a second time
2025-09-27 14:34:03.833[ERROR][SYSTEM][TRANSFER]Error in bulk upsert: ERROR: ON CONFLICT DO UPDATE command cannot affect row a second time
2025-09-27 14:33:28.043[ERROR][SYSTEM][TRANSFER]CRITICAL ERROR: Bulk upsert failed for chunk 770 in table crm.prospect_lists_prospects: ERROR: ON CONFLICT DO UPDATE command cannot affect row a second time
2025-09-27 14:33:28.043[ERROR][SYSTEM][TRANSFER]Error in bulk upsert: ERROR: ON CONFLICT DO UPDATE command cannot affect row a second time
2025-09-27 14:31:23.005[ERROR][TRANSFER][performBulkUpsert]Skipping problematic record in batch: ERROR: current transaction is aborted, commands ignored until end of

carpeta de pending

mecanismo para para el thread desde la ui 

poder reseatear los threads tambien 

valgrind --tool=callgrind ./tu_programa
suma de total de rows 


SELECT SUM(last_offset) AS total_last_offset
FROM metadata.catalog
WHERE status != 'SKIP' AND status = 'LISTENING_CHANGES';


SELECT SUM(table_size) AS total_data
FROM metadata.catalog


2025-09-27 15:04:39.829[ERROR][SYSTEM][TRANSFER]CRITICAL ERROR: Bulk upsert failed for chunk 1715 in table crm.leads: ERROR: ON CONFLICT DO UPDATE command cannot affect row a second time
2025-09-27 15:04:39.829[ERROR][SYSTEM][TRANSFER]Error in bulk upsert: ERROR: ON CONFLICT DO UPDATE command cannot affect row a second time
2025-09-27 15:03:36.469[ERROR][SYSTEM][TRANSFER]CRITICAL ERROR: Bulk upsert failed for chunk 1178 in table crm.leads: ERROR: ON CONFLICT DO UPDATE command cannot affect row a second time
2025-09-27 15:03:36.468[ERROR][SYSTEM][TRANSFER]Error in bulk upsert: ERROR: ON CONFLICT DO UPDATE command cannot affect row a second time
2025-09-27 15:03:17.075[ERROR][SYSTEM][TRANSFER]CRITICAL ERROR: Bulk upsert failed for chunk 1021 in table crm.leads: ERROR: ON CONFLICT DO UPDATE command cannot affect row a second time
2025-09-27 15:03:17.075[ERROR][SYSTEM][TRANSFER]Error in bulk upsert: ERROR: ON CONFLICT DO UPDATE command cannot affect row a second time
2025-09-27 15:02:09.336[ERROR][SYSTEM][TRANSFER]CRITICAL ERROR: Bulk upsert failed for chunk 461 in table crm.leads: ERROR: ON CONFLICT DO UPDATE command cannot affect row a second time
2025-09-27 15:02:09.336[ERROR][SYSTEM][TRANSFER]Error in bulk upsert: ERROR: ON CONFLICT DO UPDATE command cannot affect row a second time



veo que el codigo hace eso que me imagino que son incrementales pero no lo veo subir en el last_offset? ni el pk 

INSERT INTO "scrubber_service_jetu_cr"."transaction" ("id", "merchant_id", "password", "session_id", "user_name", "user_number", "player_status", "reg_date", "reg_ip_address", "reg_device_id", "first_name", "last_name", "email", "address1", "city", "province", "postal_code", "country", "phone1", "dob", "id_type", "id_value", "gender", "first_dep_date", "first_with_date", "dep_count", "with_count", "current_balance", "pay_method_type", "dl_min", "dl_max", "dl_weekly", "dl_monthly", "trans_id", "billing_first_name", "billing_last_name", "billing_email", "billing_address1", "billing_city", "billing_province", "billing_postal_code", "billing_country", "billing_phone1", "billing_dob", "billing_id_type", "billing_id_value", "billing_gender", "bin", "last_digits", "card_hash", "amount", "currency", "time", "status", "ip", "device_id", "io_bb", "quantity", "local_time", "source", "website", "custom_variable", "created_at", "updated_at", "score", "transaction_type", "gateway_settings_id") VALUES ('12403800', 'CDN-2pt


► Currently Processing: crm.contacts_users [MariaDB] (0/0 - 0%) - Status: FULL_LOAD
necesito un trigger o algo que realmente me capture la que se esta pasando !


2025-09-27 17:09:59.628[INFO][SYSTEM][TRANSFER]Executing bulk upsert for chunk 926
2025-09-27 17:09:59.628[INFO][SYSTEM][TRANSFER]Columns for bulk upsert: "id","business_process_id","customer_account","document_alias","cardnumber","required","doc_on_file","description","created_at","updated_at","encrypted"
2025-09-27 17:09:59.628[INFO][SYSTEM][TRANSFER]Preparing bulk upsert for chunk 926 with 1000 rows
2025-09-27 17:09:59.628[INFO][SYSTEM][TRANSFER]Retrieved chunk 926 with 1000 rows for document_service_v2_jetu_cr.customer_required_docs_new
2025-09-27 17:09:59.623[INFO][SYSTEM][TRANSFER]Executing data transfer query for chunk 926
2025-09-27 17:09:59.623[INFO][SYSTEM][TRANSFER]Processing chunk 926 for table document_service_v2_jetu_cr.customer_required_docs_new (size: 1000, offset: 0)
2025-09-27 17:09:59.623[INFO][SYSTEM][TRANSFER]Updated last_processed_pk to 951709 for table document_service_v2_jetu_cr.customer_required_docs_new
2025-09-27 17:09:59.622[INFO][SYSTEM][SYSTEM]Updated target count to 926000 and current offset to 0 after processing chunk 925
2025-09-27 17:09:59.622[INFO][SYSTEM][TRANSFER]Successfully processed chunk 925 with 1000 rows for document_service_v2_jetu_cr.customer_required_docs_new
2025-09-27 17:09:59.599[INFO][SYSTEM][TRANSFER]Executing bulk upsert for chunk 925
2025-09-27 17:09:59.599[INFO][SYSTEM][TRANSFER]Columns for bulk upsert: "id","business_process_id","customer_account","document_alias","cardnumber","required","doc_on_file","description","created_at","updated_at","encrypted"
2025-09-27 17:09:59.599[INFO][SYSTEM][TRANSFER]Preparing bulk upsert for chunk 925 with 1000 rows
2025-09-27 17:09:59.598[INFO][SYSTEM][TRANSFER]Retrieved chunk 925 with 1000 rows for document_service_v2_jetu_cr.customer_required_docs_new
2025-09-27 17:09:59.594[INFO][SYSTEM][TRANSFER]Executing data transfer query for chunk 925
2025-09-27 17:09:59.594[INFO][SYSTEM][TRANSFER]Processing chunk 925 for table document_service_v2_jetu_cr.customer_required_docs_new (size: 1000, offset: 0)
2025-09-27 17:09:59.593[INFO][SYSTEM][TRANSFER]Updated last_processed_pk to 950655 for table document_service_v2_jetu_cr.customer_required_docs_new
2025-09-27 17:09:59.593[INFO][SYSTEM][SYSTEM]Updated target count to 925000 and current offset to 0 after processing chunk 924
2025-09-27 17:09:59.593[INFO][SYSTEM][TRANSFER]Successfully processed chunk 924 with 1000 rows for document_service_v2_jetu_cr.customer_required_docs_new
2025-09-27 17:09:59.569[INFO][SYSTEM][TRANSFER]Executing bulk upsert for chunk 924
2025-09-27 17:09:59.569[INFO][SYSTEM][TRANSFER]Columns for bulk upsert: "id","business_process_id","customer_account","document_alias","cardnumber","required","doc_on_file","description","created_at","updated_at","encrypted"
2025-09-27 17:09:59.569[INFO][SYSTEM][TRANSFER]Preparing bulk upsert for chunk 924 with 1000 rows
2025-09-27 17:09:59.569[INFO][SYSTEM][TRANSFER]Retrieved chunk 924 with 1000 rows for document_service_v2_jetu_cr.customer_required_docs_new
2025-09-27 17:09:59.564[INFO][SYSTEM][TRANSFER]Executing data transfer query for chunk 924
2025-09-27 17:09:59.564[INFO][SYSTEM][TRANSFER]Processing chunk 924 for table document_service_v2_jetu_cr.customer_required_docs_new (size: 1000, offset: 0)
2025-09-27 17:09:59.564[INFO][SYSTEM][TRANSFER]Updated last_processed_pk to 949605 for table document_service_v2_jetu_cr.customer_required_docs_new
2025-09-27 17:09:59.564[INFO][SYSTEM][SYSTEM]Updated target count to 924000 and current offset to 0 after processing chunk 923
2025-09-27 17:09:59.564[INFO][SYSTEM][TRANSFER]Successfully processed chunk 923 with 1000 rows for document_service_v2_jetu_cr.customer_required_docs_new
2025-09-27 17:09:59.540[INFO][SYSTEM][TRANSFER]Executing bulk upsert for chunk 923
2025-09-27 17:09:59.540[INFO][SYSTEM][TRANSFER]Columns for bulk upsert: "id","business_process_id","customer_account","document_alias","cardnumber","required","doc_on_file","description","created_at","updated_at","encrypted"
2025-09-27 17:09:59.540[INFO][SYSTEM][TRANSFER]Preparing bulk upsert for chunk 923 with 1000 rows
2025-09-27 17:09:59.540[INFO][SYSTEM][TRANSFER]Retrieved chunk 923 with 1000 rows for document_service_v2_jetu_cr.customer_required_docs_new
2025-09-27 17:09:59.535[INFO][SYSTEM][TRANSFER]Executing data transfer query for chunk 923
2025-09-27 17:09:59.535[INFO][SYSTEM][TRANSFER]Processing chunk 923 for table document_service_v2_jetu_cr.customer_required_docs_new (size: 1000, offset: 0)
2025-09-27 17:09:59.535[INFO][SYSTEM][TRANSFER]Updated last_processed_pk to 948558 for table document_service_v2_jetu_cr.customer_required_docs_new
2025-09-27 17:09:59.534[INFO][SYSTEM][SYSTEM]Updated target count to 923000 and current offset to 0 after processing chunk 922
2025-09-27 17:09:59.534[INFO][SYSTEM][TRANSFER]Successfully processed chunk 922 with 1000 rows for document_service_v2_jetu_cr.customer_required_docs_new
2025-09-27 17:09:59.511[INFO][SYSTEM][TRANSFER]Executing bulk upsert for chunk 922
2025-09-27 17:09:59.511[INFO][SYSTEM][TRANSFER]Columns for bulk upsert: "id","business_process_id","customer_account","document_alias","cardnumber","required","doc_on_file","description","created_at","updated_at","encrypted"
2025-09-27 17:09:59.510[INFO][SYSTEM][TRANSFER]Preparing bulk upsert for chunk 922 with 1000 rows
2025-09-27 17:09:59.510[INFO][SYSTEM][TRANSFER]Retrieved chunk 922 with 1000 rows for document_service_v2_jetu_cr.customer_required_docs_new
2025-09-27 17:09:59.506[INFO][SYSTEM][TRANSFER]Executing data transfer query for chunk 922
2025-09-27 17:09:59.506[INFO][SYSTEM][TRANSFER]Processing chunk 922 for table document_service_v2_jetu_cr.customer_required_docs_new (size: 1000, offset: 0)
2025-09-27 17:09:59.505[INFO][SYSTEM][TRANSFER]Updated last_processed_pk to 947514 for table document_service_v2_jetu_cr.customer_required_docs_new
2025-09-27 17:09:59.505[INFO][SYSTEM][SYSTEM]Updated target count to 922000 and current offset to 0 after processing chunk 921

Eso para que es ? 



17:12:09.289[INFO][SYSTEM][MONITORING]Catalog cleanup completed successfully
2025-09-27 17:12:09.289[INFO][SYSTEM][DATABASE]Catalog cleanup completed
2025-09-27 17:12:09.289[INFO][SYSTEM][DATABASE]Cluster name updates completed
2025-09-27 17:12:08.573[INFO][SYSTEM][MONITORING]Starting catalog cleanup
2025-09-27 17:12:08.573[INFO][SYSTEM][MONITORING]MariaDB catalog sync completed successfully
2025-09-27 17:12:08.572[INFO][SYSTEM][DATABASE]Table size for account_service_jetu.accounts: 0
[UNKNOWN][UNKNOWN] ^
[UNKNOWN][UNKNOWN]LINE 1: SELECT table_rows FROM information_schema.tables WHERE table...
2025-09-27 17:12:08.572[WARNING][SYSTEM][DATABASE]Could not get table size for account_service_jetu.accounts: ERROR: column "table_rows" does not exist
2025-09-27 17:12:08.572[INFO][SYSTEM][DATABASE]PK Detection Results for account_service_jetu.accounts: hasPK=true, pkStrategy=PK, pkColumns=["id"], candidateColumns=["id","player_id"]
2025-09-27 17:12:08.571[INFO][SYSTEM][DATABASE]Found PK column: id
2025-09-27 17:12:08.571[INFO][SYSTEM][DATABASE]PK detection query returned 1 rows
2025-09-27 17:12:08.571[INFO][SYSTEM][DATABASE]Executing PK detection query: SELECT COLUMN_NAME FROM information_schema.KEY_COLUMN_USAGE WHERE TABLE_SCHEMA = 'account_service_jetu' AND TABLE_NAME = 'accounts' AND CONSTRAINT_NAME = 'PRIMARY' ORDER BY ORDINAL_POSITION;
2025-09-27 17:12:08.571[INFO][SYSTEM][DATABASE]Detecting PK for table: account_service_jetu.accounts
2025-09-27 17:12:08.546[INFO][SYSTEM][DATABASE]Found 1266 MariaDB connection(s)
2025-09-27 17:12:08.541[INFO][SYSTEM][MONITORING]MSSQL catalog sync completed successfully
2025-09-27 17:12:08.541[WARNING][SYSTEM][DATABASE]No MSSQL connections found in catalog
2025-09-27 17:12:08.541[INFO][SYSTEM][DATABASE]Found 0 MSSQL connections
2025-09-27 17:12:08.540[INFO][SYSTEM][MONITORING]PostgreSQL catalog sync completed successfully
2025-09-27 17:12:08.540[WARNING][SYSTEM][DATABASE]No PostgreSQL source connections found in catalog
2025-09-27 17:12:08.540[INFO][SYSTEM][DATABASE]Found 0 PostgreSQL source connections
2025-09-27 17:12:08.532[INFO][SYSTEM][MONITORING]Starting PostgreSQL catalog sync
2025-09-27 17:12:08.532[INFO][SYSTEM][MONITORING]Starting MSSQL catalog sync



2025-09-27 17:12:41.167[INFO][SYSTEM][MONITORING]Validating MariaDB table: sportsbook_services.contest_configuration
2025-09-27 17:12:41.160[INFO][SYSTEM][MONITORING]Validating MariaDB table: rule_service_jetu_cr.mid_redirections
2025-09-27 17:12:40.879[INFO][SYSTEM][MONITORING]Validating MariaDB table: deposit_service_jetu_cr.manual_adjustment_customers
2025-09-27 17:12:40.872[INFO][SYSTEM][MONITORING]Validating MariaDB table: document_service_v2_jetu_cr.business_documents
2025-09-27 17:12:40.867[INFO][SYSTEM][MONITORING]Validating MariaDB table: deposit_service_jetu_cr.mid_standard_error_message
2025-09-27 17:12:40.862[INFO][SYSTEM][MONITORING]Validating MariaDB table: deposit_service_jetu_cr.migrations
2025-09-27 17:12:40.314[INFO][SYSTEM][MONITORING]PostgreSQL transfer cycle completed successfully in 0 seconds



2025-09-27 17:12:39.342[INFO][SYSTEM][MONITORING]MariaDB catalog sync completed successfully
2025-09-27 17:12:39.341[INFO][SYSTEM][DATABASE]Table size for account_service_jetu.accounts: 0
[UNKNOWN][UNKNOWN] ^
[UNKNOWN][UNKNOWN]LINE 1: SELECT table_rows FROM information_schema.tables WHERE table...
2025-09-27 17:12:39.341[WARNING][SYSTEM][DATABASE]Could not get table size for account_service_jetu.accounts: ERROR: column "table_rows" does not exist
2025-09-27 17:12:39.341[INFO][SYSTEM][DATABASE]PK Detection Results for account_service_jetu.accounts: hasPK=true, pkStrategy=PK, pkColumns=["id"], candidateColumns=["id","player_id"]
2025-09-27 17:12:39.340[INFO][SYSTEM][DATABASE]Found PK column: id


2025-09-27 17:13:56.275[INFO][TRANSFER][1266/1266]account_service_jetu.vip_brand_players (status: LISTENING_CHANGES)
2025-09-27 17:13:56.275[INFO][TRANSFER][1265/1266]betcenter.temp_contest (status: LISTENING_CHANGES)
2025-09-27 17:13:56.275[INFO][TRANSFER][1264/1266]deposit_service_jetu_cr.profiles (status: LISTENING_CHANGES)
2025-09-27 17:13:56.275[INFO][TRANSFER][1263/1266]crm.calls_contacts (status: LISTENING_CHANGES)
2025-09-27 17:13:56.275[INFO][TRANSFER][1262/1266]RAFService.Messaging (status: LISTENING_CHANGES)
2025-09-27 17:13:56.275[INFO][TRANSFER][1261/1266]reports.ProcessorList (status: LISTENING_CHANGES)
2025-09-27 17:13:56.275[INFO][TRANSFER][1260/1266]documents_service_jetu_cr.document_business_processes (status: LISTENING_CHANGES)
2025-09-27 17:13:56.275[INFO][TRANSFER][1259/1266]document_service_v2_jetu_cr.document_business_processes (status: LISTENING_CHANGES)
2025-09-27 17:13:56.275[INFO][TRANSFER][1258/1266]rule_service_jetu_cr.mid_processors (status: LISTENING_CHANGES)
2025-09-27 17:13:56.275[INFO][TRANSFER][1257/1266]rule_service_jetu_cr.definitions_multi (status: LISTENING_CHANGES)
2025-09-27 17:13:56.275[INFO][TRANSFER][1256/1266]promotions_service_jetu_cr_v2._947_PromotionTemplateBrands (status: LISTENING_CHANGES)
2025-09-27 17:13:56.275[INFO][TRANSFER][1255/1266]RAFService.PromoCode (status: LISTENING_CHANGES)
2025-09-27 17:13:56.275[INFO][TRANSFER][1254/1266]account_service_jetu.settings (status: LISTENING_CHANGES)
2025-09-27 17:13:56.275[INFO][TRANSFER][1253/1266]promotions_service_jetu_cr_v2.migrations (status: LISTENING_CHANGES)
2025-09-27 17:13:56.275[INFO][TRANSFER][1252/1266]scrubber_service_jetu_cr.scrubber_settings (status: LISTENING_CHANGES)
2025-09-27 17:13:56.275[INFO][TRANSFER][1251/1266]sportsbook_services.league_translations (status: LISTENING_CHANGES)
2025-09-27 17:13:56.275[INFO][TRANSFER][1250/1266]rule_service_jetu_cr.mid_comments (status: LISTENING_CHANGES)
2025-09-27 17:13:56.275[INFO][TRANSFER][1249/1266]crm.temp_to_be_deleted (status: LISTENING_CHANGES)

para que hace eso ?



2025-09-27 17:14:44.318[INFO][SYSTEM][TRANSFER]Executing data transfer query for chunk 170
2025-09-27 17:14:44.318[INFO][SYSTEM][TRANSFER]Processing chunk 170 for table crm.AFF_TEMP (size: 1000, offset: 169000)
2025-09-27 17:14:44.317[INFO][SYSTEM][TRANSFER]Updating last_offset to 169000 for table crm.AFF_TEMP
2025-09-27 17:14:44.317[INFO][SYSTEM][SYSTEM]Updated target count to 169000 and current offset to 169000 after processing chunk 169
2025-09-27 17:14:44.317[INFO][SYSTEM][TRANSFER]Successfully processed chunk 169 with 1000 rows for crm.AFF_TEMP
2025-09-27 17:14:44.292[INFO][SYSTEM][TRANSFER]Executing bulk upsert for chunk 169
2025-09-27 17:14:44.292[INFO][SYSTEM][TRANSFER]Columns for bulk upsert: "id","created","url","sent_to","agent","referrer","redirection_id","ip","module_id","group_id"
2025-09-27 17:14:44.292[INFO][SYSTEM][TRANSFER]Preparing bulk upsert for chunk 169 with 1000 rows
2025-09-27 17:14:44.292[INFO][SYSTEM][TRANSFER]Retrieved chunk 169 with 1000 rows for crm.AFF_TEMP
2025-09-27 17:14:44.189[INFO][SYSTEM][TRANSFER]Executing data transfer query for chunk 169
2025-09-27 17:14:44.189[INFO][SYSTEM][TRANSFER]Processing chunk 169 for table crm.AFF_TEMP (size: 1000, offset: 168000)
2025-09-27 17:14:44.188[INFO][SYSTEM][TRANSFER]Updating last_offset to 168000 for table crm.AFF_TEMP
2025-09-27 17:14:44.188[INFO][SYSTEM][SYSTEM]Updated target count to 168000 and current offset to 168000 after processing chunk 168
2025-09-27 17:14:44.188[INFO][SYSTEM][TRANSFER]Successfully processed chunk 168 with 1000 rows for crm.AFF_TEMP
2025-09-27 17:14:44.162[INFO][SYSTEM][TRANSFER]Executing bulk upsert for chunk 168
2025-09-27 17:14:44.162[INFO][SYSTEM][TRANSFER]Columns for bulk upsert: "id","created","url","sent_to","agent","referrer","redirection_id","ip","module_id","group_id"
2025-09-27 17:14:44.162[INFO][SYSTEM][TRANSFER]Preparing bulk upsert for chunk 168 with 1000 rows
2025-09-27 17:14:44.162[INFO][SYSTEM][TRANSFER]Retrieved chunk 168 with 1000 rows for crm.AFF_TEMP
2025-09-27 17:14:44.060[INFO][SYSTEM][TRANSFER]Executing data transfer query for chunk 168
2025-09-27 17:14:44.060[INFO][SYSTEM][TRANSFER]Processing chunk 168 for table crm.AFF_TEMP (size: 1000, offset: 167000)
2025-09-27 17:14:44.060[INFO][SYSTEM][TRANSFER]Updating last_offset to 167000 for table crm.AFF_TEMP
2025-09-27 17:14:44.059[INFO][SYSTEM][SYSTEM]Updated target count to 167000 and current offset to 167000 after processing chunk 167
2025-09-27 17:14:44.059[INFO][SYSTEM][TRANSFER]Successfully processed chunk 167 with 1000 rows for crm.AFF_TEMP
2025-09-27 17:14:44.034[INFO][SYSTEM][TRANSFER]Executing bulk upsert for chunk 167
2025-09-27 17:14:44.034[INFO][SYSTEM][TRANSFER]Columns for bulk upsert: "id","created","url","sent_to","agent","referrer","redirection_id","ip","module_id","group_id"
2025-09-27 17:14:44.034[INFO][SYSTEM][TRANSFER]Preparing bulk upsert for chunk 167 with 1000 rows
2025-09-27 17:14:44.034[INFO][SYSTEM][TRANSFER]Retrieved chunk 167 with 1000 rows for crm.AFF_TEMP
2025-09-27 17:14:43.933[INFO][SYSTEM][TRANSFER]Executing data transfer query for chunk 167
2025-09-27 17:14:43.932[INFO][SYSTEM][TRANSFER]Processing chunk 167 for table crm.AFF_TEMP (size: 1000, offset: 166000)
2025-09-27 17:14:43.932[INFO][SYSTEM][TRANSFER]Updating last_offset to 166000 for table crm.AFF_TEMP

si va bien pero cuando lo rebotee ? luego de que procesa una full_load como que se queda atrapado haciendo incrementales y deltas



► Currently Processing: crm.AFF_TEMP [MariaDB] (518,000/967,109 - 53.6%) - Status: FULL_LOAD
la query si funciona pero tiene que ser cualquier status no solo ese 



SELECT COUNT(*) - COUNT(DISTINCT ctid) FROM "promotions_service_jetu_cr_v2"."PrizeTemplateRules";


INSERT INTO "crm"."AFF_TEMP" (
    "id",
    "created",
    "url",
    "sent_to",
    "agent",
    "referrer",
    "redirection_id",
    "ip",
    "module_id",
    "group_id"
  )
VALUES (
    '56211213',
    '2020-01-27 23:20:56',
    '/2203/',
    'NO REDIRECT SENT TO PATH',
    'Mozilla/5.0 (iPhone; CPU iPhone OS 13_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.4 Mobile/15E148 Safari/604.1',
    NULL,
    '380',
    '72.211.102.170',
    '0',
    '1'
  ),
  (
    '56211214',
    '2020-01-27 23:20:56',
    '/2802/sserve.popads.netserve.popads.net/',
    'NO REDIRECT SENT TO PATH',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.79 Safari/537.36',
    'https://serve.popads.net/s?cid=6044257&iuid=1103775437&ts=1580185245&ps=1185730831&pw=473&pl=%21k2K628t%2FNTXR1fMkYS5gaD4Ufb%2FmsjmCBFjsPi8HFplYDwiMA9hlMB%2BqhOtSmaMPITDqHL4aztHk3ESlZuiIreN1hby6Grmi7PoHReYTFsKBMU59mG1s0vVfbohsQQmCb4t14CLCIt1rNGjWYOzHrUMme8ll41fpSyrcvqkRFUAx%2BybfA43gDq4YivUX13fpVa7eR4aRbO0WlrOTc%2FQyxv6dUST21iy6gwvsDs950Q43lneajMEIONOnmj90Q3pqPVJiu%2BhJ6BBlv0mA0CcXAze%2B5

    cuando hace eso significa que esta haciendo incrementales etc no ?  bueno no porque hace esa query pero esta en FULL_LOAD no es mas rapido hacer CSV ? igual va a muy rapido asi asi que no me preocupo

     Currently Processing: crm.AFF_TEMP [MariaDB] (981,000/967,109 - 101.4%) - Status: FULL_LOAD
      Currently Processing: crm.contacts_users [MariaDB] (0/0 - 0%) - Status: FULL_LOAD si no que se queda pegado ahi mucho tiempo en la logica que pasa cuando termina una full_load? 

     termina una full_load y no continua con la otra


     RESPUESTA A TU PREGUNTA:
LA LÓGICA DE FULL_LOAD ESTÁ EN EL NIVEL CORRECTO:
✅ NIVEL 1 (Loop principal) - Es donde debe estar:
Verificación de status FULL_LOAD
Truncate (si es necesario)
Luego pasa al data transfer
❌ NIVEL 2 (Loop interno) - NO es donde debe estar:
Solo procesa chunks de datos
No maneja lógica de status
EL PROBLEMA REAL:
El problema NO es la ubicación de la lógica FULL_LOAD. El problema es que:
Nivel 1: Procesa tabla FULL_LOAD → hace truncate ✅
Nivel 1: Continúa al data transfer ✅
Nivel 2: Loop interno se queda "stuck" por errores de transacciones ❌
Nivel 1: NUNCA sale del loop interno, NUNCA procesa la siguiente tabla ❌
SOLUCIÓN:
El manejo de errores debe estar en el NIVEL 2 (loop interno) para que cuando haya un error de transacción abortada:
Haga rollback de la transacción
Continue con el siguiente chunk
NO se quede "stuck" en una sola tabla
Permita que el NIVEL 1 continúe con la siguiente tabla
La lógica de FULL_LOAD está bien ubicada, el problema es el manejo de errores en el loop interno.


¡**EXACTO**! Has identificado perfectamente el problema.

## **EL FALLO ESTÁ EN EL NIVEL 2:**

### **NIVEL 2 - DONDE OCURREN LOS DROPS/INSERTS/UPDATES:**

```cpp
while (hasMoreData) {  // ← Loop de chunks
    // 1. Query a MariaDB (SELECT)
    results = executeQueryMariaDB(mariadbConn, selectQuery);
    
    // 2. DROPS/INSERTS/UPDATES en PostgreSQL ← AQUÍ ESTÁ EL PROBLEMA
    performBulkUpsert(pgConn, results, columnNames, columnTypes, ...);
    
    // 3. Si hay error → se queda "stuck"
}
```

### **¿QUÉ PASA EN `performBulkUpsert`?**

Dentro de `performBulkUpsert` se hacen todas las operaciones de base de datos:

1. **INSERT** masivos
2. **UPDATE** masivos  
3. **UPSERT** (INSERT + UPDATE)
4. **Transacciones** complejas

### **EL ERROR QUE VISTE:**

```
ERROR: current transaction is aborted, commands ignored until end of transaction block
```

**Esto significa:**

1. ✅ **Query SELECT** a MariaDB funciona bien
2. ❌ **INSERT/UPDATE** a PostgreSQL falla
3. ❌ **Transacción se aborta**
4. ❌ **Sistema se queda "stuck"** porque no maneja el rollback correctamente
5. ❌ **NUNCA sale del while loop**
6. ❌ **NUNCA procesa la siguiente tabla FULL_LOAD**

### **FLUJO DEL PROBLEMA:**

```
Tabla 1: crm.AFF_NEW_TOKEN (FULL_LOAD)
├── ✅ Truncate OK
├── ✅ SELECT de MariaDB OK
├── ❌ INSERT/UPDATE a PostgreSQL FALLA
├── ❌ Transacción abortada
├── ❌ Se queda "stuck" aquí
└── ❌ NUNCA llega a procesar Tabla 2: crm.contacts_users
```

### **SOLUCIÓN:**

**El manejo de errores debe estar en `performBulkUpsert` para:**

1. **Detectar** transacciones abortadas
2. **Hacer rollback** inmediatamente
3. **Continuar** con el siguiente chunk
4. **NO quedarse "stuck"** en una sola tabla
5. **Permitir** que el NIVEL 1 procese la siguiente tabla

**Has identificado perfectamente la causa raíz del problema: el NIVEL 2 se queda "stuck" en las operaciones de INSERT/UPDATE/DROP cuando fallan las transacciones de PostgreSQL.**

¿Quieres que revise el código de `performBulkUpsert` para ver exactamente cómo manejar estos errores de transacciones abortadas?


Sí, exactamente. **En el NIVEL 1 ya está hecho correctamente**. Vamos a confirmarlo:


[1 tool called]

## **CONFIRMACIÓN: NIVEL 1 YA ESTÁ HECHO CORRECTAMENTE ✅**

### **NIVEL 1 (Loop principal) - COMPLETO:**

```cpp
for (auto &table : tables) {  // ← Loop de tablas
    if (table.status == "FULL_LOAD") {
        // ✅ 1. Verifica status FULL_LOAD
        // ✅ 2. Revisa last_offset
        // ✅ 3. Hace truncate (si es necesario)
        // ✅ 4. CONTINÚA al data transfer (línea 1168)
    } else if (table.status == "RESET") {
        // ✅ 5. Maneja RESET correctamente
        // ✅ 6. Hace continue (línea 1165) - salta al siguiente
    }
    
    // ✅ 7. Data transfer para TODAS las tablas (líneas 1168-1395)
    // ✅ 8. Actualización de status a LISTENING_CHANGES (líneas 1469-1488)
    // ✅ 9. Cierra conexión y continúa con siguiente tabla (línea 1500)
}
```

### **EL PROBLEMA ESTÁ EN EL NIVEL 2:**

**❌ NIVEL 2 (Loop interno) - PROBLEMA:**
```cpp
while (hasMoreData) {  // ← Loop de chunks
    // ✅ Procesa chunk
    performBulkUpsert(...);  // ← AQUÍ se queda "stuck"
    
    // ❌ Si hay error de transacción abortada:
    // ❌ NO hace rollback correctamente
    // ❌ NO continúa al siguiente chunk
    // ❌ Se queda "stuck" en esta tabla
    // ❌ NUNCA sale del while loop
    // ❌ NUNCA permite que NIVEL 1 continúe
}
```

## **RESUMEN:**

### **✅ NIVEL 1 - YA ESTÁ PERFECTO:**
- Maneja FULL_LOAD correctamente
- Hace truncate cuando es necesario
- Continúa al data transfer
- Actualiza status a LISTENING_CHANGES
- Permite continuar con la siguiente tabla

### **❌ NIVEL 2 - AQUÍ ESTÁ EL PROBLEMA:**
- Se queda "stuck" en `performBulkUpsert`
- No maneja correctamente errores de transacciones abortadas
- No hace rollback y continúa
- Bloquea el progreso a la siguiente tabla

**La solución es arreglar el manejo de errores en el NIVEL 2, específicamente en `performBulkUpsert` cuando hay transacciones abortadas.**

¿Quieres que revise el código de `performBulkUpsert` para ver cómo manejar mejor los errores de transacciones abortadas?

Excelente pregunta. Vamos a analizar exactamente dónde se debe manejar la lógica de FULL_LOAD.


[1 tool called]

Ahora voy a ver la estructura completa del loop principal:


[1 tool called]

## **ESTRUCTURA DE LOOPS - ANÁLISIS:**

### **LOOP PRINCIPAL (Nivel 1):**
```cpp
for (auto &table : tables) {  // ← AQUÍ está el loop de tablas
    // Procesa UNA tabla a la vez
    // Si es FULL_LOAD → hace truncate (si es necesario)
    // Luego CONTINÚA al data transfer
}
```

### **LOOP INTERNO (Nivel 2):**
```cpp
while (hasMoreData) {  // ← AQUÍ está el loop de chunks
    // Procesa chunks de datos
    // Si hay error → se queda "stuck"
}
```

## **RESPUESTA A TU PREGUNTA:**

### **LA LÓGICA DE FULL_LOAD ESTÁ EN EL NIVEL CORRECTO:**

**✅ NIVEL 1 (Loop principal)** - Es donde debe estar:
- Verificación de status FULL_LOAD
- Truncate (si es necesario)
- Luego pasa al data transfer

**❌ NIVEL 2 (Loop interno)** - NO es donde debe estar:
- Solo procesa chunks de datos
- No maneja lógica de status

### **EL PROBLEMA REAL:**

El problema **NO es la ubicación** de la lógica FULL_LOAD. El problema es que:

1. **Nivel 1**: Procesa tabla FULL_LOAD → hace truncate ✅
2. **Nivel 1**: Continúa al data transfer ✅  
3. **Nivel 2**: Loop interno se queda "stuck" por errores de transacciones ❌
4. **Nivel 1**: NUNCA sale del loop interno, NUNCA procesa la siguiente tabla ❌

### **SOLUCIÓN:**

**El manejo de errores debe estar en el NIVEL 2 (loop interno)** para que cuando haya un error de transacción abortada:

1. **Haga rollback** de la transacción
2. **Continue** con el siguiente chunk
3. **NO se quede "stuck"** en una sola tabla
4. **Permita** que el NIVEL 1 continúe con la siguiente tabla

**La lógica de FULL_LOAD está bien ubicada, el problema es el manejo de errores en el loop interno.**

¿Quieres que revise específicamente cómo manejar los errores de transacciones abortadas en el loop interno?

SELECT SUM(table_size) AS total_data
FROM metadata.catalog


select count(status) from metadata.catalog
where status = 'LISTENING_CHANGES'

select count(status) from metadata.catalog
where status = 'NO_DATA'

select count(active) from metadata.catalog
where active = true



SELECT
  ( (SELECT COUNT(*) FROM metadata.catalog WHERE status = 'LISTENING_CHANGES')
  + (SELECT COUNT(*) FROM metadata.catalog WHERE status = 'NO_DATA')
  - (SELECT COUNT(*) FROM metadata.catalog WHERE active = true)
  ) AS final_result;




